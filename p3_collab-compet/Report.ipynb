{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Collaboration and Competition\n",
    "\n",
    "### Algorithm Multi-Agent Deep Deterministic Policy Gradient for N agents (MADDPG)\n",
    "\n",
    "1. Theory:\n",
    "\n",
    "DDPG:\n",
    "\n",
    "Basically, the actor learns to optimize the policy with the maximum state-action estimate for a given state, thus, following a detreministic policy, as opposed to a stochastic one. Again, this is due to maximizing the estimate for a state-action value. Thus, one needs to optimize:\n",
    "\n",
    "$$argmax_{\\mu}E_{S \\sim D}[Q(s, \\mu(s; \\theta_{\\mu}); \\theta_{Q})]$$  \n",
    "\n",
    "where $D=\\{(s, a, r, s^{'}, d)_{0}, ..., (s, a, r, s^{'}, d)_{t}, ..., (s, a, r, s^{'}, d)_{T}\\}$ is the buffer for the replay-memory.\n",
    "\n",
    "On the other hand, the critic learns to approximate the optimal state-value function, $Q^{*}(s, a; \\theta_{Q})$ where $a = \\mu(s; \\theta_{\\mu})$ (detreministic policy that maximizes the probability of taking action $a$ given one is in state $s$). For stability, one uses a target function approximator and tries to learns how to maximize it:\n",
    "\n",
    "$$argmin_{\\theta_{Q}}E_{S,A \\sim D}[(Q(S_{t}, A_{t}; \\theta_{Q}) - y_{t})^{2}]$$\n",
    "\n",
    "where $y_{t} = r(S_{t}, A_{t}) + \\gamma * Q(S_{t + 1}, A_{t + 1}; \\theta_{Q})$\n",
    "\n",
    "The algorithm uses soft-updating for added stability (just as in the DQN algorithm):\n",
    "\n",
    "$$\\theta_{\\mu}^{target} = \\tau * \\theta_{\\mu} + (1 - \\tau) * \\theta_{\\mu}^{target}$$\n",
    "$$\\theta_{Q}^{target} = \\tau * \\theta_{Q} + (1 - \\tau) * \\theta_{Q}^{target}$$\n",
    "\n",
    "\n",
    "\n",
    "2. Algorithm:\n",
    "\n",
    "**for** episode = 1 to num_episodes **do**\n",
    "    \n",
    "Initialize a random process N for action exploration\n",
    "    \n",
    "Receive initial observations, $x$ where $x=\\{o_{0}, o_{1}, ..., o_{T}\\}$\n",
    "    \n",
    "**for** t=1 to trajectory_size **do**\n",
    "    \n",
    "For each agent i, select $a_{i} = \\mu(o_{i}; \\theta_{\\mu}) + N_{t}$\n",
    "\n",
    "Execute $a=(a_{1}, ..., a_{N})$ and get reward $r$ and next observations $x^{'}$\n",
    "\n",
    "Store $(x, a, r, x^{'})$ in $D$\n",
    "\n",
    "$x = x^{'}$\n",
    "\n",
    "**for** agent i=1 to N **do**\n",
    "\n",
    "Sample batch $B=(x^{j}, a^{j}, r^{j}, x^{'j})$ from $D$\n",
    "\n",
    "Update critic by $argmin_{\\theta_{Q}}L(\\theta_{Q})=\\frac{1}{|B|}\\sum_{j}(y^{j} - Q_{i}^{\\mu}(x^{j}, a_{1}^{j}, ..., a_{N}^{j}; \\theta_{Q}))^{2}$\n",
    " where $y^{j}=r_{i}^{j} + \\gamma Q_{i}^{\\mu^{'}}(x^{'j}, a_{1}^{'}, ..., a_{N}^{'})|_{a_{k}^{'}=\\mu^{'}_{k}(o_{k}^{j};\\theta_{\\mu})}$\n",
    "\n",
    "Update actor by  $argmax_{\\theta_{\\mu}}\\frac{1}{|B|}\\sum_{j}\\nabla_{\\theta_{\\mu_{i}}}\\mu_{i}(o_{i}^{j};\\theta_{\\mu})\\nabla_{a_{i}}Q_{i}^{\\mu_{i}}(x^{j}, a_{1}^{j}, ..., a_{N}^{j};\\theta_{Q})|_{a_{i}=\\mu_{i}(o_{i}^{j};\\theta_{\\mu})}$\n",
    "\n",
    "**end for**\n",
    "\n",
    "Soft-update for each agent $i$:\n",
    "\n",
    "$\\theta_{\\mu}^{target} = \\tau * \\theta_{\\mu} + (1 - \\tau) * \\theta_{\\mu}^{target}$\n",
    "\n",
    "$\\theta_{Q}^{target} = \\tau * \\theta_{Q} + (1 - \\tau) * \\theta_{Q}^{target}$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**end for**\n",
    "\n",
    "\n",
    "3. Parameters:\n",
    "\n",
    "$seed = 1$\n",
    "\n",
    "$BatchSize = 1000$\n",
    "\n",
    "$MemorySize = 1e6$\n",
    "\n",
    "$\\gamma = 0.95$\n",
    "\n",
    "$lr_{actor} = 1e-3$\n",
    "\n",
    "$lr_{critic} = 1e-3$\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Actor: 3 fully connected layers (`nn.Linear`) with a tanh activation on the output to bound the output between -1 and 1. There is one batch normalization layer after the first fully connected one (`nn.BatchNorm1d`).\n",
    "\n",
    "![alt](images/actor_architecture.png)\n",
    "\n",
    "Critic: 3 fully connected layers where the second fully connected layer accepts as input (output_fc1 + action). There is one batch normalization layer after the first fully connected one.\n",
    "\n",
    "![alt](images/critic_architecture.png)\n",
    "\n",
    "All other activation functions are `F.leaky_relu`, in order to avoid dead neurons when learning.\n",
    "\n",
    "### Training results\n",
    "\n",
    "![alt](images/plot_scores.png)\n",
    "\n",
    "### Ideas for future work\n",
    "\n",
    "I would spawn few of those pairs in parallel processes and will have them update master weights. The architecture of these \"nodes\" will be in a distributed fashion, thus, perhaps getting rid of the replay memory and it might converge faster and better. I would aslo try a different algorithm such as AlphaGo or AlphaZero, which had great success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
