{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "### Algorithm (DDPG)\n",
    "\n",
    "1. Theory:\n",
    "\n",
    "Basically, the actor learns to optimize the policy with the maximum state-action estimate for a given state, thus, following a detreministic policy, as opposed to a stochastic one. Again, this is due to maximizing the estimate for a state-action value. Thus, one needs to optimize:\n",
    "\n",
    "$$argmax_{\\mu}E_{S \\sim D}[Q(s, \\mu(s; \\theta_{\\mu}); \\theta_{Q})]$$  \n",
    "\n",
    "where $D=\\{(s, a, r, s^{'}, d)_{0}, ..., (s, a, r, s^{'}, d)_{t}, ..., (s, a, r, s^{'}, d)_{T}\\}$ is the buffer for the replay-memory.\n",
    "\n",
    "On the other hand, the critic learns to approximate the optimal state-value function, $Q^{*}(s, a; \\theta_{Q})$ where $a = \\mu(s; \\theta_{\\mu})$ (detreministic policy that maximizes the probability of taking action $a$ given one is in state $s$). For stability, one uses a target function approximator and tries to learns how to maximize it:\n",
    "\n",
    "$$argmin_{\\theta_{Q}}E_{S,A \\sim D}[(Q(S_{t}, A_{t}; \\theta_{Q}) - y_{t})^{2}]$$\n",
    "\n",
    "where $y_{t} = r(S_{t}, A_{t}) + \\gamma * Q(S_{t + 1}, A_{t + 1}; \\theta_{Q})$\n",
    "\n",
    "The algorithm uses soft-updating for added stability (just as in the DQN algorithm):\n",
    "\n",
    "$$\\theta_{\\mu}^{target} = \\tau * \\theta_{\\mu} + (1 - \\tau) * \\theta_{\\mu}^{target}$$\n",
    "$$\\theta_{Q}^{target} = \\tau * \\theta_{Q} + (1 - \\tau) * \\theta_{Q}^{target}$$\n",
    "\n",
    "\n",
    "\n",
    "2. Algorithm:\n",
    "\n",
    "Input: $\\theta_{Q}^{initial}$, $\\theta_{mu}^{initial}$, $D$\n",
    "\n",
    "Set target parameters $\\theta_{Q}^{target} = \\theta_{Q}$, $\\theta_{\\mu}^{target} = \\theta_{\\mu}$\n",
    "\n",
    "**Repeat**\n",
    "\n",
    "Observe state $s$ and choose action $a = clip(\\mu(s; \\theta_{\\mu}) + \\epsilon)$ where $\\epsilon \\sim N(0, 1)$\n",
    "\n",
    "Execute action $a$\n",
    "\n",
    "Observe next state $s^{'}$, reward $r$ and *done* signal (if $s^{'}$ is terminal)\n",
    "\n",
    "Store $(s, a, r, s^{'}, d)$ in $D$\n",
    "\n",
    "**if** $d$ is true **then** reset the environment\n",
    "\n",
    "**if** it is time to update **then**\n",
    "\n",
    "**for** however many updates **do**\n",
    "\n",
    "Sample, at random, from $D$, $B = \\{(s, a, r, s^{'}, d)\\}$\n",
    "\n",
    "Compute each $y_{t} = r(S_{t}, A_{t}) + \\gamma * Q(S_{t + 1}, \\mu(S_{t + 1}; \\theta_{\\mu}^{target}); \\theta_{Q}^{target})$\n",
    "\n",
    "Update critic, $\\nabla_{\\theta_{Q}}\\frac{1}{|B|}\\sum_{(S_{t}, A_{t}, R_{t + 1}, S_{t + 1}, d_{t + 1}) \\in B}(Q(S_{t}, A_{t}; \\theta_{Q}) - y_{t})^{2}$\n",
    "\n",
    "Update actor, $\\nabla_{\\theta_{mu}}\\frac{1}{|B|}\\sum_{t=1}^{B}Q(S_{t}, \\mu(S_{t}; \\theta_{mu}), \\theta_{Q})$\n",
    "\n",
    "Soft-update with\n",
    "\n",
    "$\\theta_{\\mu}^{target} = \\tau * \\theta_{\\mu} + (1 - \\tau) * \\theta_{\\mu}^{target}$\n",
    "\n",
    "$\\theta_{Q}^{target} = \\tau * \\theta_{Q} + (1 - \\tau) * \\theta_{Q}^{target}$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**end if**\n",
    "    \n",
    "**until** convergence\n",
    "\n",
    "3. Parameters:\n",
    "\n",
    "$seed = 1$\n",
    "\n",
    "$BatchSize = 128$\n",
    "\n",
    "$MemorySize = 1e5$\n",
    "\n",
    "$\\gamma = 0.99$\n",
    "\n",
    "$lr_{actor} = 1e-3$\n",
    "\n",
    "$lr_{critic} = 1e-3$\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Actor: 3 fully connected layers (`nn.Linear`) with a tanh activation on the output to bound the output between -1 and 1. There is one batch normalization layer after the first fully connected one (`nn.BatchNorm1d`).\n",
    "\n",
    "![alt](imgs/actor_architecture.png)\n",
    "\n",
    "Critic: 3 fully connected layers where the second fully connected layer accepts as input (output_fc1 + action). There is one batch normalization layer after the first fully connected one.\n",
    "\n",
    "![alt](imgs/critic_architecture.png)\n",
    "\n",
    "All other activation functions are `F.leaky_relu`, in order to avoid dead neurons when learning.\n",
    "\n",
    "### Training results\n",
    "\n",
    "![alt](imgs/plot_scores.png)\n",
    "\n",
    "### Ideas for future work\n",
    "\n",
    "Given the parallel agent environment, I would like to try out `A3C` or `D4PG`. I believe that these would work well. Anything that exploits the multiple agents would benefit greatly.\n",
    "\n",
    "I would also like to try Trust Region Policy Optimization (TRPO), as it seems fitting for the continuous space.\n",
    "\n",
    "Another improvement I could intriduce would be to prioritize the replay memory as many papers do. Not all states would generate equal contributions to the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
