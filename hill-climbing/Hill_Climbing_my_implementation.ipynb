{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low:  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "High:  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "States:  Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Actions:  Discrete(2)\n",
      "State vector size:  4\n",
      "Actions vector size:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "# Properties of the environment\n",
    "# vector of 4\n",
    "state_space = env.observation_space\n",
    "\n",
    "print('Low: ', state_space.low)\n",
    "print('High: ', state_space.high)\n",
    "\n",
    "# vector of 2\n",
    "action_space = env.action_space\n",
    "\n",
    "print('States: ', state_space)\n",
    "print('Actions: ', action_space)\n",
    "\n",
    "state_size = state_space.shape[0]\n",
    "action_size = action_space.n\n",
    "print('State vector size: ', state_size)\n",
    "print('Actions vector size: ', action_size)\n",
    "\n",
    "class SimplePolicy():\n",
    "    def __init__(self, env, state_size, action_size, gamma):\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Note: More efficient to use 1D array (1 is added for the bias)\n",
    "        self.w = 1e-4 * np.random.rand(state_size, action_size)\n",
    "        \n",
    "        # Note: Used in order to decide on the adaptive random radius (adaptive scale hill climbing algorithm)\n",
    "        self.last_reward = -np.Inf\n",
    "        self.best_w = self.w\n",
    "        \n",
    "        self.up_fraction = 2\n",
    "        self.down_fraction = 0.5\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return np.argmax(self.softmax(self.w.T.dot(state)))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / sum(np.exp(x))\n",
    "    \n",
    "    # Note: Perturb with adaptive Gaussian noise\n",
    "    def update(self, reward, sigma):\n",
    "        if reward < self.last_reward:\n",
    "            sigma = min(2, sigma * self.up_fraction)\n",
    "            self.w = self.best_w + sigma * np.random.rand(self.state_size, self.action_size)\n",
    "        else:\n",
    "            # Note: We risk vanishing \"gradients\"\n",
    "            sigma = max(1e-3, sigma * self.down_fraction)\n",
    "            self.w += sigma * np.random.rand(self.state_size, self.action_size)\n",
    "            \n",
    "            self.best_w = self.w\n",
    "            self.last_reward = reward\n",
    "        \n",
    "        return sigma\n",
    "        \n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 200.00\n",
      "Environment solved in 1 episodes!\tAverage Score: 200.00\n"
     ]
    }
   ],
   "source": [
    "def run(env, policy, nepisodes):\n",
    "    average_scores = deque(maxlen=100)\n",
    "    sigma = 0.1\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for episode_idx in range(1, nepisodes + 1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_rewards = []\n",
    "        while True:\n",
    "            action = policy.forward(state)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            # Note: Update at every run and not at each episode\n",
    "            # Rt+1 = SUM(from k=0 -> inf)[ (gamma**k) * Rt+k+1 ]\n",
    "            cummulative_reward = np.sum([ (gamma**idx) * episode_rewards[idx] for idx in range(0, len(episode_rewards)) ])\n",
    "\n",
    "            sigma = policy.update(cummulative_reward, sigma)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Note: I might update here\n",
    "        \n",
    "        scores.append(sum(episode_rewards))\n",
    "        average_scores.append(sum(episode_rewards))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_idx, np.mean(average_scores)), end='')\n",
    "        \n",
    "        if np.mean(average_scores) >= 195.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(max(1, episode_idx - 100), np.mean(average_scores)))\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "policy = SimplePolicy(env, state_size, action_size, gamma)\n",
    "scores = run(env, policy, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "state = env.reset()\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        action = policy.forward(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNotes:\\n\\nI have been updating at every episode run, thus, the weights are updated way faster. But they seem unstable this way, even though sometimes, the task is solved in 1 episode.\\nIt seems like it is very dependent on the initialization of the weights.\\nIt seems also that the sigma does not influence that much the episodes in which the policy is learnt, or converged, as long as it is greater than 0.1.\\n\\nPerhaps a better approach, as in the udacity example, is to do it after each run, once per iteration. I have seen, after revision, that their learning is more stable.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Notes:\n",
    "\n",
    "I have been updating at every episode run, thus, the weights are updated way faster. But they seem unstable this way, even though sometimes, the task is solved in 1 episode.\n",
    "It seems like it is very dependent on the initialization of the weights.\n",
    "It seems also that the sigma does not influence that much the episodes in which the policy is learnt, or converged, as long as it is greater than 0.1.\n",
    "\n",
    "Perhaps a better approach, as in the udacity example, is to do it after each run, once per iteration. I have seen, after revision, that their learning is more stable.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
